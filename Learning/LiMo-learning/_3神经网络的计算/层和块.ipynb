{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 层和块\n",
    "\n",
    "在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由层组（groups of layers）的重复模式组成。\n",
    "\n",
    "块（block）可以描述单个层、由多个层组成的组件或整个模型本身。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从编程的角度来看，块由类（class）表示。 它的任何子类都必须定义一个将其输入转换为输出的前向传播函数， 并且必须存储任何必需的参数。 注意，有些块不需要任何参数。 最后，为了计算梯度，块必须具有反向传播函数。 在定义我们自己的块时，由于自动微分（在 2.5节 中引入） 提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0505, -0.0474,  0.0757,  0.1586,  0.2414,  0.0592, -0.0632, -0.0161,\n",
       "          0.2122,  0.1710],\n",
       "        [ 0.0819, -0.0512,  0.0100,  0.2408,  0.1187,  0.0768, -0.0035, -0.0966,\n",
       "          0.1796,  0.3016]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0044, -0.1676, -0.1447,  0.0454,  0.0049, -0.0062,  0.3201,  0.0645,\n",
       "          0.0580, -0.0283],\n",
       "        [-0.0604, -0.2061, -0.1676,  0.0732, -0.1363, -0.0512,  0.2932,  0.1524,\n",
       "          0.0736,  0.1109]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):        # 继承nn.model类\n",
    "    def __init__(self) -> None:\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params  ,net(x)相当于调用__call__方法，虽然在这里没有实现，也可以调用\n",
    "        super().__init__()  # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        self.hidden = nn.Linear(20,256)     # 隐藏层\n",
    "        self.out = nn.Linear(256,10)        # 输出层\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self,X):\n",
    "         # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "         return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "# 实例化\n",
    "net = MLP()\n",
    "print(X.shape)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义顺序快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0039, -0.1294,  0.0252,  0.1198,  0.0634, -0.2415,  0.0104, -0.1573,\n",
       "          0.0184,  0.3332],\n",
       "        [-0.0654, -0.4052,  0.0707,  0.2182,  0.1216, -0.2018, -0.0186, -0.0924,\n",
       "          0.1075,  0.2124]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args) -> None:\n",
    "        super().__init__()\n",
    "        for idx,module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict  self._modules: Dict[str, Optional['Module']] = OrderedDict()        \n",
    "            self._modules[str(idx)] = module\n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)      #MySequential的用法与之前为Sequential类编写的代码相同 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在前向传播中执行代码\n",
    "\n",
    "Sequential类使模型构造变得简单，允许我们组合新的架构，而不必定义自己的类。\n",
    "\n",
    "并不是所有架构都是简单的顺序架构，当需要更强的灵活性时，我们需要定义自己的块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=True)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "    def forward(self,X):\n",
    "        X = self.linear(X)\n",
    "        \n",
    "        # 这里相当于实现了一个隐藏层，这个隐藏层的参数是由我们自己指定的\n",
    "        X = F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        \n",
    "        # 复用全连接层，相当于两个全连接层共享参数。对经过relu函数激活后的值在进行一次线性变换\n",
    "        X = self.linear(X)\n",
    "        \n",
    "        # 控制流，在输出返回值之前，运行了一个while循环。在L1范数大于1的条件下，将向量除以2\n",
    "        while X.abs().sum() >1:\n",
    "            X /=2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1200, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1078, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.不带参数的层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面我们定义了在前向传播中执行代码。这里类似。下面的CenteredLayer类要从其输入中减去均值。 要构建它，我们只需继承基础层类并实现前向传播功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self) -> None:     \n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        return X-X.mean()\n",
    "\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.0536e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将自定义的层作为组件合并到更加复杂的模型中\n",
    "\n",
    "net = nn.Sequential(nn.Linear(8,128),CenteredLayer())\n",
    "Y = net(torch.rand(4,8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.带参数的层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "自定义一个全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3271,  0.0967,  0.3938],\n",
       "        [ 0.0694,  0.9654, -0.5794],\n",
       "        [ 0.2892,  0.8120,  0.4311],\n",
       "        [-0.4123, -0.2198,  0.6118],\n",
       "        [ 1.2341,  1.3351, -0.7560]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units:int, units:int) -> None:\n",
    "        \"\"\"初始化参数\n",
    "\n",
    "        Args:\n",
    "            in_units (int): 输入\n",
    "            units (int): 输出\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units,units)) # 权重矩阵\n",
    "        self.bias = nn.Parameter(torch.randn(units,))       # 偏置矩阵\n",
    "    def forward(self,X):\n",
    "        linear = torch.matmul(X,self.weight.data)+self.bias.data\n",
    "        return F.relu(linear)\n",
    "linear = MyLinear(5,3)\n",
    "linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1755, 0.6996, 0.1179],\n",
       "        [0.6051, 0.5888, 0.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.7210],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47e86d731e077963188d400b641a1f5cee6401b89b8a1175acb1a082248e2517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
