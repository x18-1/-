{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力分数\n",
    "\n",
    "<img src = \"photo/attention_score.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 掩蔽SoftMax\n",
    "\n",
    "softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。 \n",
    "\n",
    "例如，为了高效处理小批量数据集， 某些文本序列被填充了没有意义的特殊词元。 为了仅将有意义的词元作为值来获取注意力汇聚， 我们可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。 \n",
    "\n",
    "通过这种方式，我们可以在下面的masked_softmax函数中 实现这样的掩蔽softmax操作， 其中任何超出有效长度的位置都被掩蔽并置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X,valid_lens):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        X (_type_): _description_\n",
    "        valid_lens (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)     # dim=-1相当于最后一个维度，相当于每一行进行操作：https://zhuanlan.zhihu.com/p/525276061\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:       # 如果维度是1\n",
    "            valid_lens = torch.repeat_interleave(valid_lens,shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)     # 相当于展平成一维\n",
    "        \n",
    "        # 最后一个轴上被隐蔽的元素使用一个非常大的负值进行替换，从而其softmax输出为0\n",
    "        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                              value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5442, 0.4558, 0.0000, 0.0000],\n",
       "         [0.5351, 0.4649, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3948, 0.2849, 0.3203, 0.0000],\n",
       "         [0.2800, 0.4219, 0.2980, 0.0000]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个批量，前两列元素是有效的，后面的全是0\n",
    "\n",
    "第二个批量，前散列元素是有效的，后面的全是0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3294, 0.2930, 0.3776, 0.0000]],\n",
       "\n",
       "        [[0.6012, 0.3988, 0.0000, 0.0000],\n",
       "         [0.1290, 0.2965, 0.2559, 0.3185]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加性注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上文图所示：加性注意力\n",
    "\n",
    "$$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,key_size, query_size, num_hiddens, drop_out, **kwargs) -> None:\n",
    "        super(AdditiveAttention).__init__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47e86d731e077963188d400b641a1f5cee6401b89b8a1175acb1a082248e2517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
